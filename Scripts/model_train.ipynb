{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a10eb2c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91d905c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from transformers import AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe48ce16",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efc6c728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on : cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 30000  \n",
    "EMBED_DIM = 128\n",
    "NUM_LAYERS = 6 \n",
    "NUM_HEADS = 4 \n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 128  \n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nWorking on : {DEVICE}\\n\")\n",
    "\n",
    "# Data Dictory\n",
    "MODEL_SAVE_DIR = \"../Models\"\n",
    "DATA_PATH = \"../Data/shakespeare.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7700347",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07c036fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, seq_len=SEQ_LEN):\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        self.tokens = tokenizer(text)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        start = idx * self.seq_len\n",
    "        end = start + self.seq_len\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = input_ids[1:] + [0]\n",
    "        \n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65320df",
   "metadata": {},
   "source": [
    "# Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab4c08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(file_path, vocab_size):\n",
    "\n",
    "    tokenizer = lambda text: text.split()\n",
    "    counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        counter.update(tokenizer(f.read()))\n",
    "\n",
    "    vocab = {word: i for i, (word, _) in enumerate(counter.most_common(vocab_size))}\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(DATA_PATH, VOCAB_SIZE)\n",
    "word_to_id = vocab\n",
    "\n",
    "id_to_word = {i: word for word, i in vocab.items()}\n",
    "\n",
    "def tokenize(text):\n",
    "    return [word_to_id.get(word, 0) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5a15f",
   "metadata": {},
   "source": [
    "# Instanciating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd4d496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataset(DATA_PATH, tokenize)\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04a1a1",
   "metadata": {},
   "source": [
    "# Building the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2296a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, seq_len):\n",
    "\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_dim)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "                        for _ in range(num_layers)])\n",
    "                        \n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        x = self.embedding(x) + self.position_embedding(positions)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131317a",
   "metadata": {},
   "source": [
    "# Instanciating the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa07912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformer(\n",
    "\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    \n",
    "    seq_len=SEQ_LEN).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8f8f8",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b1e59e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 236/236 [00:25<00:00,  9.13it/s, loss=7.74]\n",
      "Epoch 1: 100%|██████████| 236/236 [00:15<00:00, 14.82it/s, loss=7.5] \n",
      "Epoch 2: 100%|██████████| 236/236 [00:14<00:00, 16.21it/s, loss=7.02]\n",
      "Epoch 3: 100%|██████████| 236/236 [00:13<00:00, 17.99it/s, loss=6.9] \n",
      "Epoch 4: 100%|██████████| 236/236 [00:12<00:00, 18.28it/s, loss=6.39]\n",
      "Epoch 5: 100%|██████████| 236/236 [00:12<00:00, 18.22it/s, loss=6.98]\n",
      "Epoch 6: 100%|██████████| 236/236 [00:12<00:00, 18.22it/s, loss=6.64]\n",
      "Epoch 7: 100%|██████████| 236/236 [00:12<00:00, 18.18it/s, loss=6.83]\n",
      "Epoch 8: 100%|██████████| 236/236 [00:12<00:00, 18.15it/s, loss=6.48]\n",
      "Epoch 9: 100%|██████████| 236/236 [00:13<00:00, 18.14it/s, loss=6.06]\n",
      "Epoch 10: 100%|██████████| 236/236 [00:13<00:00, 18.04it/s, loss=6.7] \n",
      "Epoch 11: 100%|██████████| 236/236 [00:13<00:00, 18.08it/s, loss=6.3] \n",
      "Epoch 12: 100%|██████████| 236/236 [00:13<00:00, 18.08it/s, loss=5.97]\n",
      "Epoch 13: 100%|██████████| 236/236 [00:13<00:00, 18.08it/s, loss=6.36]\n",
      "Epoch 14: 100%|██████████| 236/236 [00:13<00:00, 18.00it/s, loss=6.17]\n",
      "Epoch 15: 100%|██████████| 236/236 [00:13<00:00, 17.87it/s, loss=6.15]\n",
      "Epoch 16: 100%|██████████| 236/236 [00:13<00:00, 17.49it/s, loss=5.65]\n",
      "Epoch 17: 100%|██████████| 236/236 [00:13<00:00, 17.10it/s, loss=6.15]\n",
      "Epoch 18: 100%|██████████| 236/236 [00:13<00:00, 17.21it/s, loss=6.03]\n",
      "Epoch 19: 100%|██████████| 236/236 [00:14<00:00, 16.75it/s, loss=5.99]\n",
      "Epoch 20: 100%|██████████| 236/236 [00:15<00:00, 14.78it/s, loss=5.73]\n",
      "Epoch 21: 100%|██████████| 236/236 [00:16<00:00, 14.38it/s, loss=5.82]\n",
      "Epoch 22: 100%|██████████| 236/236 [00:16<00:00, 14.12it/s, loss=5.69]\n",
      "Epoch 23: 100%|██████████| 236/236 [00:17<00:00, 13.68it/s, loss=5.81]\n",
      "Epoch 24: 100%|██████████| 236/236 [00:17<00:00, 13.49it/s, loss=5.66]\n",
      "Epoch 25: 100%|██████████| 236/236 [00:17<00:00, 13.72it/s, loss=5.71]\n",
      "Epoch 26: 100%|██████████| 236/236 [00:17<00:00, 13.59it/s, loss=5.73]\n",
      "Epoch 27: 100%|██████████| 236/236 [00:17<00:00, 13.51it/s, loss=5.92]\n",
      "Epoch 28: 100%|██████████| 236/236 [00:17<00:00, 13.75it/s, loss=5.58]\n",
      "Epoch 29: 100%|██████████| 236/236 [00:16<00:00, 13.90it/s, loss=5.73]\n",
      "Epoch 30: 100%|██████████| 236/236 [00:17<00:00, 13.40it/s, loss=5.57]\n",
      "Epoch 31: 100%|██████████| 236/236 [00:18<00:00, 12.55it/s, loss=5.53]\n",
      "Epoch 32: 100%|██████████| 236/236 [00:19<00:00, 11.86it/s, loss=5.52]\n",
      "Epoch 33: 100%|██████████| 236/236 [00:19<00:00, 11.90it/s, loss=5.12]\n",
      "Epoch 34: 100%|██████████| 236/236 [00:20<00:00, 11.61it/s, loss=5.27]\n",
      "Epoch 35: 100%|██████████| 236/236 [00:20<00:00, 11.42it/s, loss=5.08]\n",
      "Epoch 36: 100%|██████████| 236/236 [00:21<00:00, 10.74it/s, loss=5.53]\n",
      "Epoch 37: 100%|██████████| 236/236 [00:21<00:00, 11.09it/s, loss=5.35]\n",
      "Epoch 38: 100%|██████████| 236/236 [00:22<00:00, 10.58it/s, loss=5.4] \n",
      "Epoch 39: 100%|██████████| 236/236 [00:22<00:00, 10.53it/s, loss=5.34]\n",
      "Epoch 40: 100%|██████████| 236/236 [00:21<00:00, 10.85it/s, loss=5.39]\n",
      "Epoch 41: 100%|██████████| 236/236 [00:21<00:00, 10.83it/s, loss=5.21]\n",
      "Epoch 42: 100%|██████████| 236/236 [00:22<00:00, 10.64it/s, loss=5.17]\n",
      "Epoch 43: 100%|██████████| 236/236 [00:22<00:00, 10.42it/s, loss=5.28]\n",
      "Epoch 44: 100%|██████████| 236/236 [00:23<00:00, 10.16it/s, loss=5.5] \n",
      "Epoch 45: 100%|██████████| 236/236 [00:23<00:00, 10.25it/s, loss=5.17]\n",
      "Epoch 46: 100%|██████████| 236/236 [00:22<00:00, 10.28it/s, loss=5.03]\n",
      "Epoch 47: 100%|██████████| 236/236 [00:23<00:00, 10.25it/s, loss=5.25]\n",
      "Epoch 48: 100%|██████████| 236/236 [00:24<00:00,  9.76it/s, loss=5.19]\n",
      "Epoch 49: 100%|██████████| 236/236 [00:24<00:00,  9.73it/s, loss=5.38]\n",
      "Epoch 50: 100%|██████████| 236/236 [00:24<00:00,  9.76it/s, loss=5.24]\n",
      "Epoch 51: 100%|██████████| 236/236 [00:24<00:00,  9.71it/s, loss=5.26]\n",
      "Epoch 52: 100%|██████████| 236/236 [00:26<00:00,  8.99it/s, loss=5.16]\n",
      "Epoch 53: 100%|██████████| 236/236 [00:24<00:00,  9.46it/s, loss=5.04]\n",
      "Epoch 54: 100%|██████████| 236/236 [00:24<00:00,  9.48it/s, loss=4.9] \n",
      "Epoch 55: 100%|██████████| 236/236 [00:24<00:00,  9.44it/s, loss=5.45]\n",
      "Epoch 56: 100%|██████████| 236/236 [00:24<00:00,  9.48it/s, loss=5.03]\n",
      "Epoch 57: 100%|██████████| 236/236 [00:24<00:00,  9.50it/s, loss=5.22]\n",
      "Epoch 58: 100%|██████████| 236/236 [00:24<00:00,  9.49it/s, loss=5.13]\n",
      "Epoch 59: 100%|██████████| 236/236 [00:25<00:00,  9.44it/s, loss=5.16]\n",
      "Epoch 60: 100%|██████████| 236/236 [00:25<00:00,  9.29it/s, loss=4.99]\n",
      "Epoch 61: 100%|██████████| 236/236 [00:26<00:00,  8.91it/s, loss=5.61]\n",
      "Epoch 62: 100%|██████████| 236/236 [00:25<00:00,  9.28it/s, loss=4.97]\n",
      "Epoch 63: 100%|██████████| 236/236 [00:25<00:00,  9.20it/s, loss=5.08]\n",
      "Epoch 64: 100%|██████████| 236/236 [00:25<00:00,  9.25it/s, loss=5.07]\n",
      "Epoch 65: 100%|██████████| 236/236 [00:25<00:00,  9.13it/s, loss=4.65]\n",
      "Epoch 66: 100%|██████████| 236/236 [00:25<00:00,  9.20it/s, loss=5.09]\n",
      "Epoch 67: 100%|██████████| 236/236 [00:26<00:00,  8.85it/s, loss=4.92]\n",
      "Epoch 68: 100%|██████████| 236/236 [00:26<00:00,  8.82it/s, loss=5.31]\n",
      "Epoch 69: 100%|██████████| 236/236 [00:26<00:00,  8.99it/s, loss=5.01]\n",
      "Epoch 70: 100%|██████████| 236/236 [00:26<00:00,  8.99it/s, loss=4.97]\n",
      "Epoch 71: 100%|██████████| 236/236 [00:26<00:00,  9.05it/s, loss=4.92]\n",
      "Epoch 72: 100%|██████████| 236/236 [00:26<00:00,  9.01it/s, loss=4.88]\n",
      "Epoch 73: 100%|██████████| 236/236 [00:26<00:00,  9.06it/s, loss=5.31]\n",
      "Epoch 74: 100%|██████████| 236/236 [00:26<00:00,  8.99it/s, loss=4.95]\n",
      "Epoch 75: 100%|██████████| 236/236 [00:27<00:00,  8.74it/s, loss=5.28]\n",
      "Epoch 76: 100%|██████████| 236/236 [00:27<00:00,  8.69it/s, loss=5.01]\n",
      "Epoch 77: 100%|██████████| 236/236 [00:27<00:00,  8.71it/s, loss=5.16]\n",
      "Epoch 78: 100%|██████████| 236/236 [00:26<00:00,  8.86it/s, loss=4.96]\n",
      "Epoch 79: 100%|██████████| 236/236 [00:27<00:00,  8.68it/s, loss=5.12]\n",
      "Epoch 80: 100%|██████████| 236/236 [00:26<00:00,  9.01it/s, loss=4.83]\n",
      "Epoch 81: 100%|██████████| 236/236 [00:26<00:00,  8.85it/s, loss=4.92]\n",
      "Epoch 82: 100%|██████████| 236/236 [00:26<00:00,  8.92it/s, loss=5.03]\n",
      "Epoch 83: 100%|██████████| 236/236 [00:26<00:00,  8.81it/s, loss=4.99]\n",
      "Epoch 84: 100%|██████████| 236/236 [00:26<00:00,  8.79it/s, loss=5.18]\n",
      "Epoch 85: 100%|██████████| 236/236 [00:26<00:00,  8.79it/s, loss=5.13]\n",
      "Epoch 86: 100%|██████████| 236/236 [00:26<00:00,  8.79it/s, loss=5.14]\n",
      "Epoch 87: 100%|██████████| 236/236 [00:26<00:00,  8.82it/s, loss=4.72]\n",
      "Epoch 88: 100%|██████████| 236/236 [00:26<00:00,  8.81it/s, loss=5.06]\n",
      "Epoch 89: 100%|██████████| 236/236 [00:26<00:00,  8.78it/s, loss=5.03]\n",
      "Epoch 90: 100%|██████████| 236/236 [00:26<00:00,  8.78it/s, loss=5.27]\n",
      "Epoch 91: 100%|██████████| 236/236 [00:26<00:00,  8.81it/s, loss=5.05]\n",
      "Epoch 92: 100%|██████████| 236/236 [00:26<00:00,  8.83it/s, loss=5.03]\n",
      "Epoch 93: 100%|██████████| 236/236 [00:26<00:00,  8.78it/s, loss=4.84]\n",
      "Epoch 94: 100%|██████████| 236/236 [00:26<00:00,  8.78it/s, loss=5.02]\n",
      "Epoch 95: 100%|██████████| 236/236 [00:27<00:00,  8.73it/s, loss=5.16]\n",
      "Epoch 96: 100%|██████████| 236/236 [00:26<00:00,  8.76it/s, loss=4.95]\n",
      "Epoch 97: 100%|██████████| 236/236 [00:26<00:00,  8.78it/s, loss=4.99]\n",
      "Epoch 98: 100%|██████████| 236/236 [00:26<00:00,  8.79it/s, loss=5]   \n",
      "Epoch 99: 100%|██████████| 236/236 [00:26<00:00,  8.80it/s, loss=4.94]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_training_steps = len(dataloader) * EPOCHS\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "model.train()\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "\n",
    "        input_ids, target_ids = batch\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        target_ids = target_ids.to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        loss = loss_fn(outputs.view(-1, VOCAB_SIZE), target_ids.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a541c3",
   "metadata": {},
   "source": [
    "# Saving the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ba600c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: ../Models\\shakespeare_LLM_20250417-123315.pth\n",
      "Settings saved: ../Models\\shakespeare_LLM_20250417-123315_settings.json\n"
     ]
    }
   ],
   "source": [
    "model_settings = {\n",
    "    \"VOCAB_SIZE\": VOCAB_SIZE,\n",
    "    \"EMBED_DIM\": EMBED_DIM,\n",
    "\n",
    "    \"NUM_LAYERS\": NUM_LAYERS,\n",
    "    \"NUM_HEADS\": NUM_HEADS,\n",
    "\n",
    "    \"HIDDEN_DIM\": HIDDEN_DIM,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "\n",
    "    \"SEQ_LEN\": SEQ_LEN,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LEARNING_RATE\": LEARNING_RATE}\n",
    "\n",
    "def save_model_settings(model, model_save_dir, settings):\n",
    "\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_filename = f\"shakespeare_LLM_{timestamp}.pth\"\n",
    "    settings_filename = f\"shakespeare_LLM_{timestamp}_settings.json\"\n",
    "\n",
    "    # Save the model's state dict\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_dir, model_filename))\n",
    "\n",
    "    # Save the hyperparameters/configuration\n",
    "    with open(os.path.join(model_save_dir, settings_filename), \"w\") as f:\n",
    "        json.dump(settings, f)\n",
    "\n",
    "\n",
    "    print(f\"Model saved: {os.path.join(model_save_dir, model_filename)}\")\n",
    "    print(f\"Settings saved: {os.path.join(model_save_dir, settings_filename)}\")\n",
    "\n",
    "save_model_settings(model, MODEL_SAVE_DIR, model_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
