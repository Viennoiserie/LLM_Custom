{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a10eb2c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d905c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0c4f8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8c66a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "logging.basicConfig(\n",
    "    \n",
    "    filename=\"training.log\",\n",
    "    filemode='a',\n",
    "\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe48ce16",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6c728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on : cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 30000  \n",
    "EMBED_DIM = 256\n",
    "NUM_LAYERS = 8 \n",
    "NUM_HEADS = 8 \n",
    "\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 128  \n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "WARMUP_STEPS = 500\n",
    "SAVE_EVERY = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nWorking on : {DEVICE}\\n\")\n",
    "\n",
    "# Data Dictory\n",
    "MODEL_SAVE_DIR = \"../Models\"\n",
    "\n",
    "STAGE_DATA_PATHS = {\n",
    "\n",
    "    \"grammar\": \"../Data/grammar.txt\",\n",
    "    \"easy\": \"../Data/easy.txt\",\n",
    "\n",
    "    \"intermediate\": \"../Data/intermediate.txt\",\n",
    "    \"advanced\": \"../Data/advanced.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7700347",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07c036fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, seq_len=SEQ_LEN):\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        self.tokens = tokenizer.encode(text).ids\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        start = idx * self.seq_len\n",
    "        end = start + self.seq_len\n",
    "        input_ids = self.tokens[start:end]\n",
    "\n",
    "        target_ids = input_ids[1:] + [0]\n",
    "        \n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65320df",
   "metadata": {},
   "source": [
    "# Training the Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab4c08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(data_path, vocab_size):\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "\n",
    "    tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04a1a1",
   "metadata": {},
   "source": [
    "# Building the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2296a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, seq_len):\n",
    "\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_dim)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim) for _ in range(num_layers)])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "\n",
    "        x = self.embedding(x) + self.position_embedding(positions)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        mask = torch.triu(torch.ones(x.size(0), x.size(0), device=x.device) * float('-inf'), diagonal=1)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, src_mask=mask)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c4dbd",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7f35658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, model_dir, name_prefix):\n",
    "\n",
    "    checkpoint = {\n",
    "\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "\n",
    "        'epoch': epoch,\n",
    "        'loss': loss}\n",
    "    \n",
    "    filename = os.path.join(model_dir, f\"{name_prefix}_checkpoint_epoch{epoch}.pth\")\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    logging.info(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def save_model_settings(model, model_save_dir, settings, stage_name, epoch):\n",
    "\n",
    "    model_filename = f\"{stage_name}_epoch_{epoch}.pth\"\n",
    "    settings_filename = f\"{stage_name}_epoch_{epoch}.json\"\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_dir, model_filename))\n",
    "\n",
    "    with open(os.path.join(model_save_dir, settings_filename), \"w\") as f:\n",
    "        json.dump(settings, f, indent=2)\n",
    "        \n",
    "    logging.info(f\"Model saved: {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8f8f8",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b1e59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_stage(stage_name, data_path, model_settings):\n",
    "\n",
    "    print(f\"\\n--- Training Stage: {stage_name.upper()} ---\")\n",
    "\n",
    "    tokenizer = train_tokenizer(data_path, VOCAB_SIZE)\n",
    "\n",
    "    dataset = TextDataset(data_path, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = SimpleTransformer(\n",
    "\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "\n",
    "        seq_len=SEQ_LEN).to(DEVICE)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    num_training_steps = len(dataloader) * EPOCHS\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=num_training_steps)\n",
    "\n",
    "    model.train()\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    patience = 5\n",
    "    trigger_times = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        loop = tqdm(dataloader, leave=True)\n",
    "\n",
    "        for batch in loop:\n",
    "\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            target_ids = target_ids.to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = loss_fn(outputs.view(-1, VOCAB_SIZE), target_ids.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            loop.set_description(f\"[{stage_name}] Epoch {epoch}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Stage: {stage_name} - Epoch {epoch} - Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "\n",
    "            best_loss = avg_loss\n",
    "            trigger_times = 0\n",
    "\n",
    "            save_model_settings(model, MODEL_SAVE_DIR, model_settings, stage_name, epoch)\n",
    "            print(\"New Best loss, model saved\\n\")\n",
    "            \n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            print(f\"No increase. Patience remaining: {trigger_times}/{patience}\")\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % SAVE_EVERY == 0:\n",
    "            save_checkpoint(model, optimizer, lr_scheduler, epoch + 1, avg_loss, MODEL_SAVE_DIR, stage_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54c5a1",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38688684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Stage: GRAMMAR ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[grammar] Epoch 0:  25%|██▍       | 209/840 [00:24<01:15,  8.37it/s, loss=6.51]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVOCAB_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m: VOCAB_SIZE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m: LEARNING_RATE}\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage, path \u001b[38;5;129;01min\u001b[39;00m STAGE_DATA_PATHS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mtrain_on_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_settings\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 57\u001b[0m, in \u001b[0;36mtrain_on_stage\u001b[1;34m(stage_name, data_path, model_settings)\u001b[0m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     55\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 57\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_settings = {\n",
    "    \n",
    "    \"VOCAB_SIZE\": VOCAB_SIZE,\n",
    "    \"EMBED_DIM\": EMBED_DIM,\n",
    "\n",
    "    \"NUM_LAYERS\": NUM_LAYERS,\n",
    "    \"NUM_HEADS\": NUM_HEADS,\n",
    "    \"HIDDEN_DIM\": HIDDEN_DIM,\n",
    "\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"SEQ_LEN\": SEQ_LEN,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "\n",
    "    \"LEARNING_RATE\": LEARNING_RATE}\n",
    "\n",
    "for stage, path in STAGE_DATA_PATHS.items():\n",
    "    train_on_stage(stage, path, model_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
